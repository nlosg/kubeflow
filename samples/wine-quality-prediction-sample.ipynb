{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874f2045-6e13-4c58-ad26-0c503dae725a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.dsl import InputPath, OutputPath\n",
    "\n",
    "@dsl.component(\n",
    "    base_image=\"python:3.11\",\n",
    "    packages_to_install=[\"requests==2.32.3\", \"pandas==2.2.2\"]\n",
    ")\n",
    "def download_dataset(\n",
    "    url: str,\n",
    "    dataset_path: OutputPath()\n",
    ") -> None:\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    from io import StringIO\n",
    "    dataset = pd.read_csv(StringIO(response.text), header=0, sep=\",\")\n",
    "    dataset.to_csv(dataset_path, index=False)\n",
    "\n",
    "@dsl.component(\n",
    "    base_image=\"python:3.11\",\n",
    "    packages_to_install=[\"pandas==2.2.2\", \"pyarrow==15.0.2\"]\n",
    ")\n",
    "def preprocess_dataset(\n",
    "    dataset: InputPath(),\n",
    "    output_file: OutputPath()\n",
    ") -> None:\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(dataset, header=0)\n",
    "    df.columns = [c.lower().replace(\" \", \"_\") for c in df.columns]\n",
    "    df.to_parquet(output_file)\n",
    "\n",
    "@dsl.component(\n",
    "    base_image=\"pytorch/pytorch:2.1.0-cuda11.8-cudnn8-devel\",\n",
    "    packages_to_install=[\n",
    "        \"pandas==2.2.2\", \"torch==2.1.0\", \"mlflow==2.15.1\", \"scikit-learn==1.5.0\", \"torch-model-archiver==0.8.2\", \"minio==7.2.5\"\n",
    "    ]\n",
    ")\n",
    "def train_pytorch_model(\n",
    "    dataset: InputPath(),\n",
    "    run_name: str,\n",
    "    model_name: str,\n",
    "    weights_path: OutputPath(),        # PyTorch weights\n",
    "    torchscript_path: OutputPath(),    # TorchScript model (.pt)\n",
    "    model_uri_path: OutputPath()       # Model artifact URI as text\n",
    ") -> None:\n",
    "    import os\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    import pandas as pd\n",
    "    import mlflow\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    import pathlib\n",
    "    import subprocess\n",
    "\n",
    "    # Load data\n",
    "    df = pd.read_parquet(dataset)\n",
    "    X = df.drop(columns=['quality']).values\n",
    "    y = df['quality'].values.reshape(-1, 1)\n",
    "\n",
    "    # Preprocess\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "    # Move to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "    X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "    # Define model\n",
    "    model = nn.Linear(X_train.shape[1], 1)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Train\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    for epoch in range(100):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Save PyTorch weights (state_dict)\n",
    "    torch.save(model.state_dict(), weights_path)\n",
    "\n",
    "    # Save TorchScript model locally\n",
    "    scripted_model = torch.jit.trace(model.cpu(), X_train.cpu()[:1])\n",
    "    scripted_model.save(torchscript_path)\n",
    "\n",
    "    # --- Archive the model using torch-model-archiver ---\n",
    "    # Create a minimal handler if needed\n",
    "    handler_code = \"\"\"\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from ts.torch_handler.base_handler import BaseHandler\n",
    "\n",
    "class WineQualityHandler(BaseHandler):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.initialized = False\n",
    "\n",
    "    def initialize(self, context):\n",
    "        # Standard TorchServe model loading for TorchScript\n",
    "        self.manifest = context.manifest\n",
    "        properties = context.system_properties\n",
    "        model_dir = properties.get(\"model_dir\")\n",
    "        serialized_file = self.manifest['model']['serializedFile']\n",
    "        model_pt_path = os.path.join(model_dir, serialized_file)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = torch.jit.load(model_pt_path, map_location=self.device)\n",
    "        self.model.eval()\n",
    "        self.initialized = True\n",
    "\n",
    "    def preprocess(self, data):\n",
    "        # Accepts JSON {\"data\": [[feature1, feature2, ...], ...]}\n",
    "        # or {\"data\": [feature1, feature2, ...]} for single sample\n",
    "        if isinstance(data, list) and len(data) > 0:\n",
    "            input_data = data[0].get('data')\n",
    "            if isinstance(input_data[0], list):\n",
    "                # Batch\n",
    "                tensor = torch.tensor(input_data, dtype=torch.float32)\n",
    "            else:\n",
    "                # Single sample\n",
    "                tensor = torch.tensor([input_data], dtype=torch.float32)\n",
    "            return tensor.to(self.device)\n",
    "        else:\n",
    "            raise ValueError(\"Input data must be a JSON list with a 'data' key.\")\n",
    "\n",
    "    def inference(self, input_tensor):\n",
    "        with torch.no_grad():\n",
    "            output = self.model(input_tensor)\n",
    "        return output\n",
    "\n",
    "    def postprocess(self, inference_output):\n",
    "        # Convert output tensor to list\n",
    "        return inference_output.cpu().numpy().tolist()\n",
    "\n",
    "    def handle(self, data, context):\n",
    "        # Main entry point for TorchServe\n",
    "        input_tensor = self.preprocess(data)\n",
    "        output = self.inference(input_tensor)\n",
    "        result = self.postprocess(output)\n",
    "        return result\n",
    "\"\"\"\n",
    "    handler_path = \"wine_quality_handler.py\"\n",
    "    with open(handler_path, \"w\") as f:\n",
    "        f.write(handler_code)\n",
    "\n",
    "    mar_name = f\"{model_name}.mar\"\n",
    "    mar_path = f\"model-store/{mar_name}\"\n",
    "    os.makedirs(\"model-store\", exist_ok=True)\n",
    "\n",
    "    # Run torch-model-archiver\n",
    "    subprocess.run([\n",
    "        \"torch-model-archiver\",\n",
    "        \"--model-name\", model_name,\n",
    "        \"--version\", \"1.0\",\n",
    "        \"--serialized-file\", str(torchscript_path),\n",
    "        \"--handler\", handler_path,\n",
    "        \"--version\", \"1.0\",\n",
    "        \"--export-path\", \"model-store\",\n",
    "        \"--force\"\n",
    "    ], check=True)\n",
    "\n",
    "    # --- TorchServe config.properties ---\n",
    "    config_dir = pathlib.Path(\"config\")\n",
    "    config_dir.mkdir(parents=True, exist_ok=True)\n",
    "    config_path = config_dir / \"config.properties\"\n",
    "    config_content = (\n",
    "        \"model_store=model-store\\n\"\n",
    "        f\"load_models={mar_name}\\n\"\n",
    "        \"enable_envvars_config=true\\n\"\n",
    "        \"disable_token_authorization=true\\n\"\n",
    "    )\n",
    "    with open(config_path, \"w\") as f:\n",
    "        f.write(config_content)\n",
    "\n",
    "    # --- Upload .mar and config.properties to MinIO ---\n",
    "    from minio import Minio\n",
    "\n",
    "    # MinIO connection settings (set via env or hardcoded for demo)\n",
    "    minio_endpoint = os.getenv(\"MINIO_ENDPOINT\", \"minio-service.kubeflow:9000\")\n",
    "    minio_access_key = os.getenv(\"MINIO_ACCESS_KEY\", \"minio\")\n",
    "    minio_secret_key = os.getenv(\"MINIO_SECRET_KEY\", \"minio123\")\n",
    "    minio_bucket = os.getenv(\"MINIO_BUCKET\", \"mlpipeline\")\n",
    "    minio_secure = False  # Set to True if using HTTPS\n",
    "\n",
    "    minio_client = Minio(\n",
    "        minio_endpoint,\n",
    "        access_key=minio_access_key,\n",
    "        secret_key=minio_secret_key,\n",
    "        secure=minio_secure\n",
    "    )\n",
    "\n",
    "    # Ensure bucket exists\n",
    "    if not minio_client.bucket_exists(minio_bucket):\n",
    "        minio_client.make_bucket(minio_bucket)\n",
    "\n",
    "    # Upload .mar to model-store/ in MinIO\n",
    "    minio_client.fput_object(\n",
    "        minio_bucket,\n",
    "        f\"model-store/{mar_name}\",\n",
    "        mar_path\n",
    "    )\n",
    "\n",
    "    # Upload config.properties to config/ in MinIO\n",
    "    minio_client.fput_object(\n",
    "        minio_bucket,\n",
    "        \"config/config.properties\",\n",
    "        str(config_path)\n",
    "    )\n",
    "\n",
    "    # Log to MLflow (optional, for experiment tracking)\n",
    "    mlflow.pytorch.autolog()\n",
    "    with mlflow.start_run(run_name=run_name) as run:\n",
    "        mlflow.log_param(\"model_type\", \"PyTorchLinearRegression\")\n",
    "        mlflow.pytorch.log_model(model, \"model\", registered_model_name=model_name)\n",
    "        model_uri = f\"{run.info.artifact_uri}/model\"\n",
    "        with open(model_uri_path, \"w\") as f:\n",
    "            f.write(model_uri)\n",
    "\n",
    "\n",
    "@dsl.component(\n",
    "    base_image=\"python:3.11\",\n",
    "    packages_to_install=[\"kserve==0.15.0\", \"kubernetes==29.0.0\", \"nvgpu==0.10.0\"]\n",
    ")\n",
    "def serve_pytorch_model(\n",
    "    service_name: str = \"wine-quality-pytorch\",\n",
    "    namespace: str = \"kubeflow-user-example-com\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Deploys a trained PyTorch model using KServe from MinIO.\n",
    "    Assumes model is stored in:\n",
    "      s3://mlpipeline/mnt/models/model-store/model.mar\n",
    "      s3://mlpipeline/mnt/models/config/config.properties\n",
    "    \"\"\"\n",
    "    from kserve import KServeClient, V1beta1InferenceService, V1beta1InferenceServiceSpec, V1beta1PredictorSpec, V1beta1TorchServeSpec\n",
    "    from kubernetes import client as k8s_client\n",
    "    from kubernetes.config import ConfigException, load_incluster_config, load_kube_config\n",
    "\n",
    "    # Load the Kubernetes configuration\n",
    "    try:\n",
    "        load_incluster_config()\n",
    "    except ConfigException:\n",
    "        load_kube_config()\n",
    "\n",
    "    kserve_client = KServeClient()\n",
    "\n",
    "    # The storage_uri points to the MinIO bucket root (adjust if you use a subfolder)\n",
    "    storage_uri = \"s3://mlpipeline/\"\n",
    "\n",
    "    isvc = V1beta1InferenceService(\n",
    "        api_version=\"serving.kserve.io/v1beta1\",\n",
    "        kind=\"InferenceService\",\n",
    "        metadata=k8s_client.V1ObjectMeta(\n",
    "            name=service_name,\n",
    "            namespace=namespace,\n",
    "            annotations={\"pipelines.kubeflow.org/owned_by_kfp_run\": \"false\", \"sidecar.istio.io/inject\": \"false\"}\n",
    "        ),\n",
    "        spec=V1beta1InferenceServiceSpec(\n",
    "            predictor=V1beta1PredictorSpec(\n",
    "                service_account_name=\"sa-minio-kserve\",\n",
    "                pytorch=V1beta1TorchServeSpec(\n",
    "                    storage_uri=storage_uri,\n",
    "                    image=\"pytorch/torchserve\",\n",
    "                    runtime_version=\"0.12.0-gpu\",\n",
    "                    resources=k8s_client.V1ResourceRequirements(\n",
    "                        limits = {\"nvidia.com/gpu\":\"1\"}\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    kserve_client.create(isvc, watch=True)\n",
    "    print(f\"Applied InferenceService '{service_name}' in namespace '{namespace}'.\")\n",
    "    print(\"Waiting for InferenceService to become ready...\")\n",
    "    kserve_client.wait_isvc_ready(name=service_name, namespace=namespace, timeout_seconds=600)\n",
    "    isvc_status = kserve_client.get(name=service_name, namespace=namespace)\n",
    "    print(f\"InferenceService '{service_name}' is ready.\")\n",
    "    print(f\"Prediction URL: {isvc_status['status']['url']}\")\n",
    "\n",
    "@dsl.pipeline(name=\"wine-quality-pytorch-pipeline\")\n",
    "def wine_quality_pipeline(\n",
    "    url: str = \"https://raw.githubusercontent.com/plotly/datasets/master/winequality-red.csv\",\n",
    "    run_name: str = \"wine-quality-pytorch-run\",\n",
    "    model_name: str = \"wine-quality-pytorch-model\"\n",
    "):\n",
    "    download_task = download_dataset(url=url)\n",
    "    preprocess_task = preprocess_dataset(dataset=download_task.outputs[\"dataset_path\"])\n",
    "    train_task = train_pytorch_model(\n",
    "        dataset=preprocess_task.outputs['output_file'],\n",
    "        run_name=run_name,\n",
    "        model_name=model_name\n",
    "    )\n",
    "    serve_task = serve_pytorch_model()\n",
    "    serve_task.after(train_task)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import kfp\n",
    "    kfp_client = kfp.Client()\n",
    "    kfp_client.create_run_from_pipeline_func(\n",
    "        wine_quality_pipeline,\n",
    "        experiment_name=\"wine_quality_test\",\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
